{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import csv\n",
    "from model import *\n",
    "import pickle\n",
    "\n",
    "\n",
    "def read_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"  \n",
    "\n",
    "print(\"device\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction(model,test_loader):\n",
    "    predictions_dict = {}  # Dictionary to store inputs and their predictions\n",
    "    total = 0\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through test dataset\n",
    "    for images in test_loader:\n",
    "        # Extract the tensor from the list (since it's a list of inputs)\n",
    "        images = images[0]  # Get the input tensor\n",
    "        \n",
    "        # Move images to the appropriate device (CPU/GPU)\n",
    "        images = images.to(device)  # If using GPU, otherwise omit or use .to('cpu')\n",
    "\n",
    "        # Forward pass to get logits/output\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            outputs = model(images)\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Iterate through the batch and associate each input with its prediction\n",
    "        for img, pred in zip(images, predicted):\n",
    "            # Store input and prediction in the dictionary\n",
    "            # If you want to store the raw image tensor, do so directly:\n",
    "            predictions_dict[img.cpu()] = pred.cpu().item()  # Store input tensor and predicted label\n",
    "\n",
    "            # If you prefer to store a simpler reference (e.g., index or image number), use:\n",
    "            # predictions_dict[f\"input_{total}\"] = pred.cpu().item()  # Use a custom key instead of the raw image\n",
    "\n",
    "        # Update total number of processed images\n",
    "        total += images.size(0)\n",
    "\n",
    "    # Now `predictions_dict` contains all the inputs and their corresponding predictions\n",
    "    # print(len(predictions_dict))\n",
    "    return predictions_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_arm_folded(test_loader,input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_arm_folded.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    # prediction_label_dict = {\n",
    "    # 0:'partially folded',\n",
    "    # 1: 'folded',\n",
    "    # }\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    # string_predictions_list = [prediction_label_dict[prediction] for prediction in predictions_list]\n",
    "    return predictions_list\n",
    "    \n",
    "def right_forearm_orientation(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_forearm_orientation_1.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_hand_position_along_body(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_hand_position_along_body.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_elbow_orientation(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_elbow_orientation.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_palm_position(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_palm_position.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_fingers_joined(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_fingers_joined.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_finger_orientation(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/multi_people_models_1/right_fingertips_orientation.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list\n",
    "\n",
    "def right_finger_closeness_to_face(test_loader, input_dim, hidden_dim, output_dim):\n",
    "    model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.load_state_dict(torch.load('/4TBHD/ISL/CodeBase/vector_models/10kmodels/right_finger_closeness_to_face.h5'))\n",
    "    predictions_dict = model_prediction(model,test_loader)\n",
    "    predictions_list = list(predictions_dict.values())\n",
    "    return predictions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  config = read_config('config.yaml')\n",
    "  input_dim =  config['inference'][\"input_dim\"]\n",
    "  hidden_dim =  config['inference'][\"hidden_dim\"]\n",
    "\n",
    "  model_list = [\"10kmodels/right_arm_folded\",\n",
    "                \"10kmodels/right_forearm_orientation\",\n",
    "                \"10kmodels/right_hand_position_along_body\",\n",
    "                \"10kmodels/right_elbow_orientation\",\n",
    "                \"10kmodels/right_palm_position\",\n",
    "                \"10kmodels/right_fingers_joined\",\n",
    "                \"multi_people_models_1/right_fingertips_orientation\",\n",
    "                \"10kmodels/right_finger_closeness_to_face\"\n",
    "                  ]\n",
    "  \n",
    "  pickle_file_save_dir = '/4TBHD/ISL/CodeBase'\n",
    "  \n",
    "  file_path = f'{pickle_file_save_dir}/seq_dataset.pkl'\n",
    "  if os.path.getsize(file_path) > 0:\n",
    "      with open(file_path, 'rb') as f:\n",
    "          classification_dict = pickle.load(f)\n",
    "  else:\n",
    "      print(\"The file is empty.\")\n",
    "  \n",
    "  # loop here for every image\n",
    "  feature_list=[]\n",
    "  for vid in classification_dict.keys():\n",
    "    vector_list = []\n",
    "    for img in range(len(classification_dict[vid])):\n",
    "      # print(\"length of classification_dict\",len(classification_dict))\n",
    "      path = classification_dict[vid][img].replace('onlyface','keypoints').replace('jpg','npy')\n",
    "      print(\"path\",path)\n",
    "      keypoint = np.load(path).reshape(1,-1)\n",
    "      z_tensor = torch.tensor(keypoint, dtype=torch.float32)\n",
    "\n",
    "      test_data = torch.utils.data.TensorDataset(z_tensor)\n",
    "      test_loader = torch.utils.data.DataLoader(test_data, 1, shuffle=False)\n",
    "      \n",
    "      right_arm_folded_list = right_arm_folded(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_arm_folded\"])\n",
    "      right_forearm_orientation_list = right_forearm_orientation(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_forearm_orientation\"])\n",
    "      right_hand_position_along_body_list = right_hand_position_along_body(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_hand_position_along_body\"])\n",
    "      right_elbow_orientation_list = right_elbow_orientation(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_elbow_orientation\"])\n",
    "      right_palm_position_list = right_palm_position(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_palm_position\"])\n",
    "      right_fingers_joined_list = right_fingers_joined(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_fingers_joined\"])\n",
    "      right_finger_orientation_list = right_finger_orientation(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_finger_orientation\"])\n",
    "      right_finger_closeness_to_face_list = right_finger_closeness_to_face(test_loader,input_dim,hidden_dim,config['output_dim_mapping'][\"right_finger_closeness_to_face\"])\n",
    "\n",
    "      feature_vector =  right_arm_folded_list + right_forearm_orientation_list + right_hand_position_along_body_list + right_elbow_orientation_list + right_palm_position_list + right_fingers_joined_list + right_finger_orientation_list + right_finger_closeness_to_face_list\n",
    "      print(\"feature_vector\",feature_vector)\n",
    "      vector_list.append(feature_vector)\n",
    "    print(\"vector_list\",len(vector_list))\n",
    "    feature_list.append(vector_list)\n",
    "    \n",
    "  \n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "vid_name = []\n",
    "for vid in classification_dict.keys():\n",
    "    labels.append(vid.split(\"_\")[1])\n",
    "    vid_name.append(vid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203\n"
     ]
    }
   ],
   "source": [
    "print(len(feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# combined_lists = [vid_name, feature_list, labels]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {'vid_name': vid_name,\n",
    "        'feature_list': feature_list,\n",
    "        'labels': labels}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save DataFrame to a pickle file\n",
    "pickle_file_path = '/4TBHD/ISL/CodeBase/Sequence_Model/sequence_data/sequence_data.pkl'\n",
    "df.to_pickle(pickle_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>feature_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishan_go_5</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ishan_go_4</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ishan_go_14</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ishan_go_17</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ishan_go_2</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vid_name                                       feature_list labels\n",
       "0   ishan_go_5  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...     go\n",
       "1   ishan_go_4  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...     go\n",
       "2  ishan_go_14  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...     go\n",
       "3  ishan_go_17  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...     go\n",
       "4   ishan_go_2  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 3, 2, 0, 0, ...     go"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pickle_file_path = '/4TBHD/ISL/CodeBase/Sequence_Model/sequence_data_train/sequence_data_2003.pkl'\n",
    "df_loaded = pd.read_pickle(pickle_file_path)\n",
    "\n",
    "# Print the loaded DataFrame\n",
    "# print(df_loaded)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_196691/4168713073.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df_loaded.groupby('labels').apply(lambda x: x.sample(n=150, random_state=42) if len(x) >= 10 else x).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "sampled_df = df_loaded.groupby('labels').apply(lambda x: x.sample(n=150, random_state=42) if len(x) >= 10 else x).reset_index(drop=True)\n",
    "\n",
    "# Display the new sampled DataFrame\n",
    "# print(sampled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path_prof = '/4TBHD/ISL/CodeBase/Sequence_Model/sequence_data_train/10signs_150vid_each_prof.pkl'\n",
    "sampled_df.to_pickle(pickle_file_path_prof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2203\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vid_name</th>\n",
       "      <th>feature_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ishan_go_5</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [0, 3, 2, 3, 1, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ishan_go_4</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [0, 3, 2, 3, 0, 1, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ishan_go_14</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 3, 0, 1, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ishan_go_17</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 2, 0, 0, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ishan_go_2</td>\n",
       "      <td>[[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 3, 0, 1, ...</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vid_name                                       feature_list labels\n",
       "0   ishan_go_5  [[2, 3, 3, 2, 0, 0, 0, 6], [0, 3, 2, 3, 1, 0, ...     go\n",
       "1   ishan_go_4  [[2, 3, 3, 2, 0, 0, 0, 6], [0, 3, 2, 3, 0, 1, ...     go\n",
       "2  ishan_go_14  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 3, 0, 1, ...     go\n",
       "3  ishan_go_17  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 2, 0, 0, ...     go\n",
       "4   ishan_go_2  [[2, 3, 3, 2, 0, 0, 0, 6], [2, 3, 2, 3, 0, 1, ...     go"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a new list\n",
    "original_list = df_loaded['feature_list']\n",
    "vid_name = df_loaded['vid_name']\n",
    "labels = df_loaded['labels']\n",
    "new_vid_list = []\n",
    "\n",
    "# Loop through the original list\n",
    "for i in range(len(original_list)):\n",
    "    new_img_list = []\n",
    "    for j in range(len(original_list[i])):\n",
    "        # Append the element to new_list only if it is not the same as the previous one\n",
    "        if j == 0 or original_list[i][j] != original_list[i][j - 1]:\n",
    "            new_img_list.append(original_list[i][j])\n",
    "    new_vid_list.append(new_img_list)\n",
    "    \n",
    "# Output the new list without consecutive duplicates\n",
    "print(len(new_vid_list))\n",
    "\n",
    "data = {'vid_name': vid_name,\n",
    "        'feature_list': new_vid_list,\n",
    "        'labels': labels}\n",
    "\n",
    "df_reduced = pd.DataFrame(data)\n",
    "df_reduced.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "i = 20\n",
    "print(len(original_list[i]))\n",
    "print(len(new_vid_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to a pickle file\n",
    "reduced_file_path = '/4TBHD/ISL/CodeBase/Sequence_Model/sequence_data_train/reduced_seq_data_2003.pkl'\n",
    "df_reduced.to_pickle(reduced_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".actrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
